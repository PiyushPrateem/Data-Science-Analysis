{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e5592d5",
   "metadata": {},
   "source": [
    "# 1.\tFirstly, replace all Missing values with relevant figures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb9ed3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "data= pd.read_csv(\"Dataset_Day7.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f2be660",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pregnancies                 0\n",
      "Glucose                     0\n",
      "BloodPressure               0\n",
      "BMI                         0\n",
      "DiabetesPedigreeFunction    0\n",
      "Age                         0\n",
      "Outcome                     0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "missing_values=data.isna().sum()\n",
    "print(missing_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1436cbd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values before replacement:\n",
      "Pregnancies                  0\n",
      "Glucose                      5\n",
      "BloodPressure               35\n",
      "BMI                         11\n",
      "DiabetesPedigreeFunction     0\n",
      "Age                          0\n",
      "Outcome                      0\n",
      "dtype: int64\n",
      "Missing values after replacement:\n",
      "Pregnancies                 0\n",
      "Glucose                     0\n",
      "BloodPressure               0\n",
      "BMI                         0\n",
      "DiabetesPedigreeFunction    0\n",
      "Age                         0\n",
      "Outcome                     0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "columns_replace=['Glucose','BloodPressure','BMI','DiabetesPedigreeFunction']\n",
    "\n",
    "for column in columns_replace:\n",
    "    data[column] = data[column].replace(0,pd.NA)\n",
    "    \n",
    "print(\"Missing values before replacement:\")\n",
    "print(data.isnull().sum())\n",
    "\n",
    "for column in columns_replace:\n",
    "    data[column].fillna(data[column].mean(), inplace = True)\n",
    "    \n",
    "print(\"Missing values after replacement:\")\n",
    "print(data.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd6b012",
   "metadata": {},
   "source": [
    "# 2.\tThen remove all existing outliers and get the final data for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e1d0299",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset shape: (768, 7)\n",
      "Cleaned dataset shape: (720, 7)\n"
     ]
    }
   ],
   "source": [
    "def remove_outliers(df,columns):\n",
    "    Q1= df[columns].quantile(0.25)\n",
    "    Q3= df[columns].quantile(0.75)\n",
    "    IQR= Q3 - Q1\n",
    "    df_clean = df[~((df[columns] < (Q1 - 1.5 * IQR)) |(df[columns] > (Q3 + 1.5 * IQR))).any(axis=1)]\n",
    "    return df_clean\n",
    "\n",
    "columns_to_check = ['Glucose', 'BloodPressure', 'BMI', 'DiabetesPedigreeFunction']\n",
    "data_clean = remove_outliers(data, columns_to_check)\n",
    "\n",
    "print(\"Original dataset shape:\", data.shape)\n",
    "print(\"Cleaned dataset shape:\", data_clean.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3346059",
   "metadata": {},
   "source": [
    "# 3.\tSplit the data into 70% training and 30% testing data. Then, create a logistic regression model with target variable as ‘Outcome’.\n",
    "a.\tPrint the default model performance metrics: Accuracy, Precision, Recall, F1Score & AIC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd688064",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94a7230f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 73.59307359307358\n",
      "Precision: 57.971014492753625\n",
      "Recall: 55.55555555555556\n",
      "F1 score: 56.73758865248227\n"
     ]
    }
   ],
   "source": [
    "X = data.drop(columns='Outcome')\n",
    "y = data['Outcome']\n",
    "\n",
    "#data into 70% and 30% testing data\n",
    "X_train, X_test, y_train, y_test= train_test_split(X, y, test_size=0.3, random_state=203)\n",
    "\n",
    "# regression model\n",
    "# Create a logistic regression model\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "#prediction on test data\n",
    "y_pred= model.predict(X_test)\n",
    "y_pred_prob = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "#calculation of performance metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test,y_pred)\n",
    "recall = recall_score(y_test,y_pred)\n",
    "f1 = f1_score(y_test,y_pred)\n",
    "\n",
    "#print performance metrics\n",
    "print(\"Accuracy:\", accuracy*100)\n",
    "print(\"Precision:\", precision*100)\n",
    "print(\"Recall:\", recall*100)\n",
    "print(\"F1 score:\", f1*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd21dcdb",
   "metadata": {},
   "source": [
    "# AIC using statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cbf23c7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.451908\n",
      "         Iterations 6\n",
      "AIC: 499.3486853177712\n"
     ]
    }
   ],
   "source": [
    "X_train_sm = sm.add_constant(X_train) # Adding a constant for the intercept\n",
    "logit_model = sm.Logit(y_train, X_train_sm)\n",
    "result = logit_model.fit()\n",
    "\n",
    "aic = result.aic\n",
    "\n",
    "# Print AIC\n",
    "print(f\"AIC:\",aic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ebb1893b",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2932973753.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Input \u001b[1;32mIn [8]\u001b[1;36m\u001b[0m\n\u001b[1;33m    ->Accuracy is about 73.59%, which means that the model correctly classifies approximately 73.593% of the instances.\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# Observations:\n",
    "->Accuracy is about 73.59%, which means that the model correctly classifies approximately 73.593% of the instances.\n",
    "#->Precision is about 57.0%, indicating that out of all the positive predictions, around 57.0% are actually positive.\n",
    "#->Recall is approximately 55.55%, meaning the model correctly identifies 55.55% of all actual positive instances.\n",
    "#->F1-Score: The F1-score of 56.73% reflects the balance between precision and recall. It is a useful metric when you need to balance both false positives and false negatives.\n",
    "#->AIC is 499.348, which is a measure used in model selection to indicate the relative quality of a statistical model. Lower AIC values generally indicate a better model fit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5fed1c",
   "metadata": {},
   "source": [
    "# 4.\tPlot a F1_score vs threshold curve. Find the threshold for which f1-score is the highest.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b18da22",
   "metadata": {},
   "outputs": [],
   "source": [
    "thresholds = np.arange(0.0, 1.0, 0.01)\n",
    "f1_scores = [f1_score(y_test, y_pred_prob >= t) for t in thresholds]\n",
    "\n",
    "# Plot F1 score vs threshold\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(thresholds, f1_scores, label='F1 Score')\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('F1 Score')\n",
    "plt.title('F1 Score vs Threshold')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Find the threshold that gives the highest F1 score\n",
    "best_threshold = thresholds[np.argmax(f1_scores)]\n",
    "best_f1_score = max(f1_scores)\n",
    "\n",
    "print(f'Best Threshold: {best_threshold}')\n",
    "print(f'Best F1 Score: {best_f1_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b7ff930",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
